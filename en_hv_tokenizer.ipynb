{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "pwd = pathlib.Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>hv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teen depression why .</td>\n",
       "      <td>depresyon sa mga tin - edyer ngaa .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what can help .</td>\n",
       "      <td>ano ang makabulig sa ila .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the compound heat shield of the saharan silver...</td>\n",
       "      <td>ang panagang sa init sang saharan silver ant .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>do you have a feast constantly .</td>\n",
       "      <td>may ara ka bala sing dalayon nga piesta .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all the days of the afflicted one are bad but ...</td>\n",
       "      <td>ang tanan nga adlaw sang napiotan malaut , apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251</th>\n",
       "      <td>i take the heavens and the earth as witnesses ...</td>\n",
       "      <td>ginahimo ko ang langit kag ang duta nga mga sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5252</th>\n",
       "      <td>while you throw all your anxiety on him , beca...</td>\n",
       "      <td>samtang ginatugyan ninyo sa iya ang tanan niny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5253</th>\n",
       "      <td>draw close to god , and he will draw close to ...</td>\n",
       "      <td>magpalapit kamo sa dios , kag magapalapit sia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5254</th>\n",
       "      <td>cleanse your hands , you sinners , and purify ...</td>\n",
       "      <td>tinlui ang inyo mga kamot , kamo nga makasasal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5255</th>\n",
       "      <td>this book of the law should not depart from yo...</td>\n",
       "      <td>indi ka mag - untat sa paghambal tuhoy sa sini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5256 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     en  \\\n",
       "0                                teen depression why .    \n",
       "1                                      what can help .    \n",
       "2     the compound heat shield of the saharan silver...   \n",
       "3                     do you have a feast constantly .    \n",
       "4     all the days of the afflicted one are bad but ...   \n",
       "...                                                 ...   \n",
       "5251  i take the heavens and the earth as witnesses ...   \n",
       "5252  while you throw all your anxiety on him , beca...   \n",
       "5253  draw close to god , and he will draw close to ...   \n",
       "5254  cleanse your hands , you sinners , and purify ...   \n",
       "5255  this book of the law should not depart from yo...   \n",
       "\n",
       "                                                     hv  \n",
       "0                  depresyon sa mga tin - edyer ngaa .   \n",
       "1                           ano ang makabulig sa ila .   \n",
       "2       ang panagang sa init sang saharan silver ant .   \n",
       "3            may ara ka bala sing dalayon nga piesta .   \n",
       "4     ang tanan nga adlaw sang napiotan malaut , apa...  \n",
       "...                                                 ...  \n",
       "5251  ginahimo ko ang langit kag ang duta nga mga sa...  \n",
       "5252  samtang ginatugyan ninyo sa iya ang tanan niny...  \n",
       "5253  magpalapit kamo sa dios , kag magapalapit sia ...  \n",
       "5254  tinlui ang inyo mga kamot , kamo nga makasasal...  \n",
       "5255  indi ka mag - untat sa paghambal tuhoy sa sini...  \n",
       "\n",
       "[5256 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data/en_hv_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "This note presents the word piece tokenization from the dataset. This is where English and Hiligaynon words are extracted to assign one hot encoding for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5256\n",
      "5256\n"
     ]
    }
   ],
   "source": [
    "en_texts = df['en'].values # get en column data\n",
    "hv_texts = df['hv'].values # get hv column data\n",
    "\n",
    "# ensure string data type\n",
    "en_samples = []\n",
    "for txt in en_texts:\n",
    "    en_samples.append(str(txt)) \n",
    "    \n",
    "hv_samples = []\n",
    "for txt in hv_texts:\n",
    "    hv_samples.append(str(txt))\n",
    "    \n",
    "print(len(en_samples))\n",
    "print(len(hv_samples))\n",
    "\n",
    "# convert to the texts to tensor\n",
    "train_en = tf.data.Dataset.from_tensor_slices(en_samples)\n",
    "train_hv = tf.data.Dataset.from_tensor_slices(hv_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create en vocabulary\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 8000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens = reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params = bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_hv.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', \"'\", ',', '-', '.', '1', '2']\n",
      "['##s', 'mag', 'paagi', 'pag', 'asawa', 'kabuhi', 'nagsiling', '##o', 'butang', 'gusto']\n",
      "['##pangayo', '##panghikot', '##plano', '##pnag', '##po', '##son', '##tawag', '##tigulang', '##tima', '##tong']\n",
      "['##1', '##2', '##c', '##j', '##q', '##v', '##x', '##z', '##¿', '##⁄']\n"
     ]
    }
   ],
   "source": [
    "print(hv_vocab[:10])\n",
    "print(hv_vocab[100:110])\n",
    "print(hv_vocab[1000:1010])\n",
    "print(hv_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "        for token in vocab:\n",
    "            print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('data/hv_vocab.txt', hv_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "    train_en.batch(1000).prefetch(2),\n",
    "    **bert_vocab_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', \"'\", ',', '-', '.', '1', '2']\n",
      "['time', 'her', 'an', '##t', 'him', 'does', 'some', 'good', 'no', 'even']\n",
      "['disciples', 'discipline', 'eliezer', 'expect', 'fellow', 'filled', 'former', 'free', 'friendships', 'grow']\n",
      "['##1', '##2', '##c', '##j', '##q', '##u', '##v', '##z', '##¿', '##⁄']\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab[:10])\n",
    "print(en_vocab[100:110])\n",
    "print(en_vocab[1000:1010])\n",
    "print(en_vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('data/en_vocab.txt', en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_tokenizer = text.BertTokenizer('data/hv_vocab.txt', **bert_tokenizer_params) # create hv tokenizer\n",
    "en_tokenizer = text.BertTokenizer('data/en_vocab.txt', **bert_tokenizer_params) # create en tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Texts:\n",
      "['teen depression why . ', 'what can help . ', 'the compound heat shield of the saharan silver ant . ']\n",
      "\n",
      "> Tokens:\n",
      "[29, 123, 291, 351, 165, 7]\n",
      "[66, 60, 115, 7]\n",
      "[38, 12, 520, 469, 576, 1377, 28, 241, 347, 421, 73, 41, 38, 28, 151, 241, 1285, 28, 347, 219, 1342, 102, 103, 7]\n"
     ]
    }
   ],
   "source": [
    "# test en tokenizer\n",
    "en_tokenizer_samples = en_samples[:3]\n",
    "\n",
    "print(\"> Texts:\")\n",
    "print(en_tokenizer_samples)\n",
    "print()\n",
    "\n",
    "# Tokenize the examples\n",
    "en_token_batch = en_tokenizer.tokenize(en_tokenizer_samples)\n",
    "# Merge the word and word-piece axes\n",
    "en_token_batch = en_token_batch.merge_dims(-2,-1)\n",
    "\n",
    "print(\"> Tokens:\")\n",
    "for ex in en_token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Texts:\n",
      "['depresyon sa mga tin - edyer ngaa . ', 'ano ang makabulig sa ila . ', 'ang panagang sa init sang saharan silver ant . ']\n",
      "\n",
      "> Tokens:\n",
      "[292, 40, 42, 652, 6, 641, 140, 7]\n",
      "[73, 39, 161, 40, 56, 7]\n",
      "[39, 68, 218, 308, 90, 40, 49, 115, 41, 40, 260, 242, 66, 127, 1311, 334, 110, 115, 7]\n"
     ]
    }
   ],
   "source": [
    "# test en tokenizer\n",
    "hv_tokenizer_samples = hv_samples[:3]\n",
    "\n",
    "print(\"> Texts:\")\n",
    "print(hv_tokenizer_samples)\n",
    "print()\n",
    "\n",
    "# Tokenize the examples\n",
    "hv_token_batch = hv_tokenizer.tokenize(hv_tokenizer_samples)\n",
    "# Merge the word and word-piece axes\n",
    "hv_token_batch = hv_token_batch.merge_dims(-2,-1)\n",
    "\n",
    "print(\"> Tokens:\")\n",
    "for ex in hv_token_batch.to_list():\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom tokenization for Translation later\n",
    "START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "\n",
    "def add_start_end(ragged):\n",
    "  count = ragged.bounding_shape()[0]\n",
    "  starts = tf.fill([count,1], START)\n",
    "  ends = tf.fill([count,1], END)\n",
    "  return tf.concat([starts, ragged, ends], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=string, numpy=\n",
       "array([b'[START] teen depression why . [END]',\n",
       "       b'[START] what can help . [END]',\n",
       "       b'[START] the compound heat shield of the saharan silver ant . [END]'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = en_tokenizer.detokenize(add_start_end(en_token_batch))\n",
    "tf.strings.reduce_join(words, separator=' ', axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_text(reserved_tokens, token_txt):\n",
    "    # Drop the reserved tokens, except for \"[UNK]\".\n",
    "    bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n",
    "    bad_token_re = \"|\".join(bad_tokens)\n",
    "\n",
    "    bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
    "    result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
    "\n",
    "    # Join them into strings.\n",
    "    result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'teen', b'depression', b'why', b'.'], [b'what', b'can', b'help', b'.'],\n",
       " [b'the', b'compound', b'heat', b'shield', b'of', b'the', b'saharan',\n",
       "  b'silver', b'ant', b'.']                                           ]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_batch = en_tokenizer.tokenize(en_tokenizer_samples).merge_dims(-2,-1)\n",
    "words = en_tokenizer.detokenize(token_batch)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'teen depression why .', b'what can help .',\n",
       "       b'the compound heat shield of the saharan silver ant .'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanup_text(reserved_tokens, words).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTokenizer(tf.Module):\n",
    "    def __init__(self, reserved_tokens, vocab_path):\n",
    "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
    "        self._reserved_tokens = reserved_tokens\n",
    "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
    "\n",
    "        file_path = pathlib.Path(vocab_path, encoding=\"utf-8\")\n",
    "\n",
    "        vocab = []\n",
    "        with file_path.open(encoding='utf-8') as file:\n",
    "            # Read and process each line\n",
    "            for line in file:\n",
    "                vocab.append(line.strip())\n",
    "                \n",
    "        self.vocab = tf.Variable(vocab)\n",
    "\n",
    "        ## Create the signatures for export:   \n",
    "\n",
    "        # Include a tokenize signature for a batch of strings. \n",
    "        self.tokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
    "\n",
    "        # Include `detokenize` and `lookup` signatures for:\n",
    "        #   * `Tensors` with shapes [tokens] and [batch, tokens]\n",
    "        #   * `RaggedTensors` with shape [batch, tokens]\n",
    "        self.detokenize.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.detokenize.get_concrete_function(\n",
    "                tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        self.lookup.get_concrete_function(\n",
    "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "        self.lookup.get_concrete_function(\n",
    "                tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
    "\n",
    "        # These `get_*` methods take no arguments\n",
    "        self.get_vocab_size.get_concrete_function()\n",
    "        self.get_vocab_path.get_concrete_function()\n",
    "        self.get_reserved_tokens.get_concrete_function()\n",
    "\n",
    "    @tf.function\n",
    "    def tokenize(self, strings):\n",
    "        enc = self.tokenizer.tokenize(strings)\n",
    "        # Merge the `word` and `word-piece` axes.\n",
    "        enc = enc.merge_dims(-2,-1)\n",
    "        enc = add_start_end(enc)\n",
    "        return enc\n",
    "\n",
    "    @tf.function\n",
    "    def detokenize(self, tokenized):\n",
    "        words = self.tokenizer.detokenize(tokenized)\n",
    "        return cleanup_text(self._reserved_tokens, words)\n",
    "\n",
    "    @tf.function\n",
    "    def lookup(self, token_ids):\n",
    "        return tf.gather(self.vocab, token_ids)\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_size(self):\n",
    "        return tf.shape(self.vocab)[0]\n",
    "\n",
    "    @tf.function\n",
    "    def get_vocab_path(self):\n",
    "        return self._vocab_path\n",
    "\n",
    "    @tf.function\n",
    "    def get_reserved_tokens(self):\n",
    "        return tf.constant(self._reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizers = tf.Module()\n",
    "tokenizers.hv = CustomTokenizer(reserved_tokens, 'data/hv_vocab.txt')\n",
    "tokenizers.en = CustomTokenizer(reserved_tokens, 'data/en_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_name = 'ted_hrlr_translate_hv_en_converter'\n",
    "tf.saved_model.save(tokenizers, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1432"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload and test\n",
    "reloaded_tokenizers = tf.saved_model.load(model_name)\n",
    "reloaded_tokenizers.en.get_vocab_size().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2,  73, 135,  69,   3]], dtype=int64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = reloaded_tokenizers.hv.tokenize(['ano himuon ko'])\n",
    "tokens.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cccc2bf72e25cd1e2c43186e636030c3e5fd2718fd9c33a254a3734b1b01d762"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
